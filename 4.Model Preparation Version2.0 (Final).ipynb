{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import lightgbm as lg\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildData():\n",
    "    df=pd.read_csv('dataset/train.csv',index_col='Index')\n",
    "    trainingSet, testSet = train_test_split(df, test_size=0.2,shuffle=True)\n",
    "    print(len(trainingSet))\n",
    "    print(len(testSet))\n",
    "    trainingSet.to_csv(\"dataset/new_train.csv\", sep=',')\n",
    "    testSet.to_csv(\"dataset/new_test.csv\", sep=',')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdf():\n",
    "    df=pd.read_csv('dataset/new_train.csv',index_col='Index')\n",
    "    df=df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    df[\"Market\"] = df[\"Market\"].astype('category')\n",
    "    df[\"Day\"] = df[\"Day\"].astype('category')\n",
    "    df[\"Stock\"] = df[\"Stock\"].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(type_,df):\n",
    "    if type_=='minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif type_=='standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif type_=='maxabs':\n",
    "        scaler= MaxAbsScaler()\n",
    "    elif type_=='robust':\n",
    "        scaler= RobustScaler()\n",
    "    elif type_=='QuantileTransformer':\n",
    "        scaler=QuantileTransformer()\n",
    "    elif type_=='Normalizer':\n",
    "        scaler=Normalizer()    \n",
    "    \n",
    "    df[['x0','x1','x2','x3A','x3B','x3C','x3D','x3E','x4','x5','x6']]=scaler.fit_transform(df[['x0','x1','x2','x3A','x3B','x3C','x3D','x3E','x4','x5','x6']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorValue(y_pred,y,wt):\n",
    "    return (sum(wt*((y_pred-y)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=getdf()\n",
    "df=encode(df)\n",
    "df=scale('standard',df)\n",
    "wt=df['Weight']\n",
    "x=df.drop(['Weight','y'],axis=1)    \n",
    "y=df['y']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "[CV] n_estimators=10 .................................................\n",
      "[CV] n_estimators=10 .................................................\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] .................................. n_estimators=10, total=  56.3s\n",
      "[CV] .................................. n_estimators=10, total=  56.6s\n",
      "[CV] ................................. n_estimators=100, total= 6.3min\n",
      "[CV] ................................. n_estimators=100, total= 6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20785636145517128 5\n",
      "{'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "X=x\n",
    "parameters = {'n_estimators':[10,100]}\n",
    "model = RandomForestRegressor()\n",
    "grid_RF = GridSearchCV(model,parameters, cv=2,verbose = 2,n_jobs=-1)\n",
    "grid_RF.fit(X, y)\n",
    "y_pred=grid_RF.predict(X)\n",
    "print(errorValue(y_pred,y,wt),5)\n",
    "print(grid_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3429572965478771 5\n"
     ]
    }
   ],
   "source": [
    "X=x\n",
    "model=RandomForestRegressor(bootstrap=True,min_samples_leaf= 5,max_features= 3,n_estimators=100)\n",
    "model.fit(X,y)\n",
    "y_pred=model.predict(X)\n",
    "print(errorValue(y_pred,y,wt),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features  Values\n",
      "x4 \t 0.1561\n",
      "x3E \t 0.104\n",
      "x3D \t 0.0866\n",
      "x5 \t 0.077\n",
      "x3A \t 0.0714\n",
      "x1 \t 0.0691\n",
      "x6 \t 0.0675\n",
      "x3C \t 0.0669\n",
      "x3B \t 0.0652\n",
      "x2 \t 0.0597\n",
      "Day \t 0.0512\n",
      "Stock \t 0.0484\n",
      "x0 \t 0.0388\n",
      "Market \t 0.038\n"
     ]
    }
   ],
   "source": [
    "# print(X.columns)\n",
    "imp_list=np.around(np.array(model.feature_importances_),4)\n",
    "feature_list=sorted(zip(imp_list, X.columns),reverse=True)\n",
    "print(\"Features  Values\")\n",
    "for item in feature_list:\n",
    "    print(item[1],\"\\t\",round(item[0],4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest gave an error of.2404,.2383,.2389 \n",
    "\n",
    "#### Best Score of .3363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x\n",
    "Y=y\n",
    "Wt=wt\n",
    "X_encode=pd.get_dummies(X, prefix=['Market'], columns=['Market'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] learning_rate=0.7 ...............................................\n",
      "[CV] ................................ learning_rate=0.7, total=  52.4s\n",
      "[CV] learning_rate=0.7 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   53.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................ learning_rate=0.7, total=  50.3s\n",
      "[CV] learning_rate=0.7 ...............................................\n",
      "[CV] ................................ learning_rate=0.7, total=  50.2s\n",
      "[CV] learning_rate=0.2 ...............................................\n",
      "[CV] ................................ learning_rate=0.2, total=  47.7s\n",
      "[CV] learning_rate=0.2 ...............................................\n",
      "[CV] ................................ learning_rate=0.2, total=  48.9s\n",
      "[CV] learning_rate=0.2 ...............................................\n",
      "[CV] ................................ learning_rate=0.2, total=  47.9s\n",
      "[CV] learning_rate=0.05 ..............................................\n",
      "[CV] ............................... learning_rate=0.05, total=  40.4s\n",
      "[CV] learning_rate=0.05 ..............................................\n",
      "[CV] ............................... learning_rate=0.05, total=  40.3s\n",
      "[CV] learning_rate=0.05 ..............................................\n",
      "[CV] ............................... learning_rate=0.05, total=  40.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5178516613707286\n"
     ]
    }
   ],
   "source": [
    "parameters = {}\n",
    "model = XGBRegressor()\n",
    "grid_XG = GridSearchCV(model,parameters, cv=3,verbose = 2)\n",
    "grid_XG.fit(X_encode, Y)\n",
    "y_pred=grid_XG.predict(X_encode)\n",
    "print(errorValue(y_pred,Y,Wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Gave an error of .010467 , (.5220 with hyper parameter tuning),.5482, .5178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=x[100000:200000]\n",
    "# X_encode=pd.get_dummies(X, prefix=['Market','Stock','Day'], columns=['Market','Stock','Day'])\n",
    "# Y=y[100000:200000]\n",
    "# Wt=wt[100000:200000]\n",
    "# y_pred=grid.predict(X_encode)\n",
    "# print(errorValue(y_pred,Y,Wt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_df=X_encode[X_encode.columns[range(11,3295)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95,whiten=True)\n",
    "X_pca = pca.fit_transform(X_encode)\n",
    "print('Original number of features:', X_encode.shape[1])\n",
    "print('Reduced number of features:', X_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So PCA didn't Work due to Memmory Error. I guess the idea to split categorical value into columns wont work in this system as dataset gets huge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=getdf()\n",
    "# df=encode(df)\n",
    "# df=scale('standard',df)\n",
    "# wt=df['Weight']\n",
    "# x=df.drop(['Weight','y'],axis=1)    \n",
    "# y=df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data_frame):\n",
    "\n",
    "    # creating new features\n",
    "    data_frame['new']  = data_frame['x3B'] - data_frame['x5']\n",
    "    data_frame['new2']  = data_frame['x3C'] - data_frame['x4']\n",
    "    \n",
    "    # scalling up \"small\" features\n",
    "    small_features_1 = ['x0','x2',\"x4\"]\n",
    "    small_features_2 = [\"x3A\",'x1', \"x3B\", \"x3C\", \"x3D\", \"x3E\", \"x5\", \"new\", \"new2\"]\n",
    "    data_frame[small_features_1]= data_frame[small_features_1]*1000\n",
    "    data_frame[small_features_2]= data_frame[small_features_2]*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering(df)\n",
    "X_train = df.drop(['y','Weight'],1)\n",
    "Y = df.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/jithin/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "100%|██████████| 5/5 [01:30<00:00, 18.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data into train lightgbm dataset\n",
    "# notice I'm scaling up the target, making first two columns as categorical features, and load weights\n",
    "train = lg.Dataset(X_train, Y, categorical_feature=[0, 1], weight=df.Weight, free_raw_data=False)\n",
    "\n",
    "# hyperparameters for the model\n",
    "parameters = {'learning_rate': '0.9'}\n",
    "\n",
    "boosts = 200\n",
    "num_ensembles = 5\n",
    "y_pred = 0.0\n",
    "\n",
    "# average 5 different models \n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    model = lg.train(parameters, train_set=train, num_boost_round=boosts + i + 2) \n",
    "    y_pred +=  model.predict(data=X_train)\n",
    "y_pred /= num_ensembles\n",
    "gc.collect()\n",
    "\n",
    "# 'num_leaves': 526, \n",
    "#  'max_bin': 650, 'feature_fraction': '0.450', \n",
    "#  'learning_rate': '0.009', 'reg_lambda': 3, 'bagging_freq': 2,\n",
    "#  'min_data_in_leaf': 142, 'colsample_bytree': '0.670', \n",
    "#  'metric': 'rmse', 'verbose': 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.260838974997406\n"
     ]
    }
   ],
   "source": [
    "print(errorValue(y_pred,Y,wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM is giving an error Value of .38226, .3843, .5292(tuning), .2608(tuning -learning rate .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM Version -Jithin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=getdf()\n",
    "df=scale('standard',df)\n",
    "wt=df['Weight']\n",
    "x=df.drop(['Weight','y'],axis=1)    \n",
    "y=df['y']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jithin/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    }
   ],
   "source": [
    "train = lg.Dataset(x, y, categorical_feature=[0, 1, 2], weight=df.Weight, free_raw_data=False)\n",
    "parameters = {'learning_rate': '0.9'}\n",
    "\n",
    "model_lgb = lg.train(params=parameters,train_set=train, num_boost_round=300)\n",
    "y_pred = model_lgb.predict(data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1509990547939315\n"
     ]
    }
   ],
   "source": [
    "print(errorValue(y_pred,Y,wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on First Run - Value is .3134\n",
    "#### on Second Run - Value is .15099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on the TEST Data now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdf_test():\n",
    "    df=pd.read_csv('dataset/new_test.csv',index_col='Index')\n",
    "    df=df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7fe65fe7db00>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jithin/anaconda3/lib/python3.6/site-packages/xgboost/core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n",
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7fe66544bc18>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jithin/anaconda3/lib/python3.6/site-packages/xgboost/core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "df=getdf_test()\n",
    "df=encode(df)\n",
    "df=scale('standard',df)\n",
    "wt=df['Weight']\n",
    "x=df.drop(['Weight','y'],axis=1)    \n",
    "y=df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17512580135567665 5\n"
     ]
    }
   ],
   "source": [
    "## Applying Random Forest Model on Test Data\n",
    "\n",
    "X=x\n",
    "y_pred=grid_RF.predict(X)\n",
    "print(errorValue(y_pred,y,wt),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1323489853611735\n"
     ]
    }
   ],
   "source": [
    "## Applying XG Boost Model on Test Data\n",
    "\n",
    "X=x\n",
    "Y=y\n",
    "Wt=wt\n",
    "\n",
    "X_encode=pd.get_dummies(X, prefix=['Market'], columns=['Market'])\n",
    "y_pred=grid_XG.predict(X_encode)\n",
    "print(errorValue(y_pred,Y,Wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1509990547939315\n"
     ]
    }
   ],
   "source": [
    "## Applying LightGBM model on Test Data\n",
    "\n",
    "df=getdf()\n",
    "df=scale('standard',df)\n",
    "wt=df['Weight']\n",
    "x=df.drop(['Weight','y'],axis=1)    \n",
    "y=df['y'] \n",
    "\n",
    "y_pred = model_lgb.predict(data=x)\n",
    "print(errorValue(y_pred,y,wt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence Result :- Light GBM model came up with the least Error Value of .15 which is great for the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and learning from this Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Project was based on a challenge which was previously attempted by the best minds in the industry with a considerable price money. This project introduced me to the concept of Boosting in particular the XG Boost and Light GBM. I got the least error by applying the Light GBM model while gradient boosting continued to give a very high error the test data. \n",
    "\n",
    "One challenge i faced was when i tried to  use get dummies to encode the categorical data of Market, Stock and Day which actualy breaks my memory. So i had to stop working on it and my idea. \n",
    "\n",
    "Further improvement. \n",
    "\n",
    "1) Categorizing the stocks (creating a unsupervised learning model and get some clusters of stock like (High Performing Stock, Low Performing stock etc) and then use this as one of the feature.\n",
    "\n",
    "2) Feature X4 and a combination of X4 with other variables as new feature (This method was used by one of the top 3 candidates for their model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
